{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ver001(x):\n",
    "    return [1/(1 + np.exp(-ele)) for ele in x]\n",
    "\n",
    "def Ver002(x):\n",
    "    return 1 / (1 + (np.exp(-x)))\n",
    "\n",
    "min_limit = 709\n",
    "def Ver003(x):\n",
    "    return 1 / (1 + np.exp(-(np.clip(x,-min_limit,None))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7310585786300049]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ver001([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ver002(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ver003([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a given numpy array to its sigmoid and its associated derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input array\n",
    "x = [[3, 1], [0, 4]] \n",
    "\n",
    "#Sample output\n",
    "# [[0.95 0.73]\n",
    "#  [0.5 0.98]] \n",
    "# [[0.05 0.2 ]\n",
    "#  [0.25 0.02]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95 0.73]\n",
      " [0.5  0.98]]\n",
      "[[0.05 0.2 ]\n",
      " [0.25 0.02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings as war\n",
    "war.filterwarnings('ignore')\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''x is a list\n",
    "       Output -> Two numpy arrays are expected to be returned both with the same dimensions as of x'''\n",
    "    \n",
    "    x = np.asarray(x)\n",
    "\n",
    "    sigmoid  = 1 / (1 + (np.exp(-x)))\n",
    "    sigmoid_dev = sigmoid * (1 - sigmoid)\n",
    "    print(sigmoid.round(2))\n",
    "    print(sigmoid_dev.round(2))\n",
    "           \n",
    "#Driver function\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q) Log loss and derivative\n",
    "\n",
    "\n",
    "Problem Statement\n",
    "\n",
    "LogLoss is defined as: −y.log( \n",
    "y\n",
    "​\n",
    " )−(1−y).log(1− \n",
    "y\n",
    "​\n",
    " ),\n",
    "where y represents the true label and  \n",
    "y\n",
    "​\n",
    "  represents the probability output by logistic regression model.\n",
    "\n",
    "\n",
    "Problem Description:\n",
    "\n",
    "Given the true labels for the ith sample is y \n",
    "i\n",
    "​\n",
    "  and z \n",
    "i\n",
    "​\n",
    " =w \n",
    "0\n",
    "​\n",
    " +∑ \n",
    "j=1\n",
    "d\n",
    "​\n",
    " w \n",
    "j\n",
    "​\n",
    " ×x \n",
    "ij\n",
    "​\n",
    " , where:\n",
    "\n",
    "1. d is the number of features used\n",
    "\n",
    "2. w \n",
    "j\n",
    "​\n",
    "  is the jth value of the weight of the model\n",
    "\n",
    "3. x \n",
    "ij\n",
    "​\n",
    "  is the jth feature value of the ith sample\n",
    "\n",
    "\n",
    "\n",
    "Find and return the logLoss and the derivative of logLoss (w.r.t to the first feature) which is defined as:\n",
    "\n",
    "\n",
    "\n",
    "dwj/dL(w)  =(yi − yi))*(−xij)\n",
    "\n",
    "\n",
    "\n",
    "Note: The values in the arrays should be rounded up to 2 decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "z = [3.11, 0.08, 0.76, 5.98, 3.05, 0.12, 8.99, 1.69, 1.75, 1.54] \n",
    "y_true = [1, 0, 1, 0, 0, 0, 1, 1, 0, 1]  \n",
    "x = [[11, 22], [39, 0], [33, 39], [1, 28], [9, 24], [19, 14], [6, 7], [28, 3], [4, 17], [35, 15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.04, 0.73, 0.38, 5.98, 3.1 , 0.75, 0.  , 0.17, 1.91, 0.19]),\n",
       " array([ -0.47,  20.28, -10.52,   1.  ,   8.59,  10.07,  -0.  ,  -4.36,\n",
       "          3.41,  -6.18]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def logloss(z, y_true, x):\n",
    "    '''z, y_true and x are lists\n",
    "       output -> Two numpy arrays are expected to be returned'''\n",
    "    \n",
    "    z = np.asarray(z)\n",
    "    y_true = np.asarray(y_true)\n",
    "    x = np.asarray(x)\n",
    "    \n",
    "    # YOUR CODE GOES HERE\n",
    "\n",
    "    y_hat = 1 / (1 + (np.exp(-z)))\n",
    "    #y_hat = y_hat.round(2)\n",
    "\n",
    "    logloss = (-1 * y_true * np.log(y_hat)) - ((1 - y_true) * np.log(1 - y_hat))\n",
    "    logloss = logloss.round(2)\n",
    "    \n",
    "    logloss_dev = (y_hat - y_true) * [x[0] for x in x]\n",
    "    logloss_dev = logloss_dev.round(2)\n",
    "\n",
    "    return logloss,logloss_dev\n",
    "\n",
    "\n",
    "\n",
    "logloss(z, y_true, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 39, 33, 1, 9, 19, 6, 28, 4, 35]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-879a04ce8a40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "x[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Description**\n",
    "\n",
    "Given train and test data in the form of lists as arguments of the function errors(), complete the function to return the lists train_errs and test_errs where train_err will consist of the errors in X_train for different values of C in the list c, and test_errs will have the errors for the test data.\n",
    " - ***C is basically a hyperparameter in the logistic regression model representing the reciprocal of regularization parameter λ. High values of C cause less regularization which leads the model to overfit on training data, whereas low values of C causes it to underfit.***\n",
    "\n",
    "Note: Here, by error we mean the percentage of misclassified points, converted to decimal.\n",
    "\n",
    "\n",
    "Sample Input:\n",
    "\n",
    "- X_train = [[93, 17], [83, 8], [80, 13], [15, 30], [63, 31], [11, 28], [73, 55], [82, 11], [50, 6], [2, 89]]\n",
    "- y_train = [0, 0, 1, 0, 1, 1, 1, 1, 0, 1]\n",
    "- X_test = [[9, 53], [6, 7], [21, 61], [65, 75], [48, 67]]\n",
    "- y_test = [0, 0, 1, 0, 0]\n",
    "- c = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "\n",
    "Sample Output:\n",
    "\n",
    "- train_errs = [0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n",
    "- test_errs = [0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#import logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def training_errors(X_train, y_train, X_test, y_test,c):\n",
    "    ''' X_train, y_train, X_test, y_test,c are all in the form of lists\n",
    "        Output -> Two lists are expected to be returned'''\n",
    "    \n",
    "    #converting all the lists into Numpy arrays\n",
    "    X_train = np.asarray(X_train)  \n",
    "    X_test = np.asarray(X_test)  \n",
    "    y_train = np.asarray(y_train)  \n",
    "    y_test = np.asarray(y_test)  \n",
    "    train_errs = list()\n",
    "    test_errs = list()\n",
    "    \n",
    "    for c_value in c:\n",
    "        #initialize the logistic regression model\n",
    "        clf = LogisticRegression(C=c_value)\n",
    "        \n",
    "        #fit the training data on the model\n",
    "        clf.fit(X_train,y_train)\n",
    "\n",
    "        #y_pred_train = clf.predict(X_train)\n",
    "        train_score = clf.score(X_train,y_train)\n",
    "        test_score = clf.score(X_test,y_test)\n",
    "        \n",
    "        #append the errors for each value of C for train and test data.\n",
    "        train_errs.append(round( 1.0 - train_score, 2))\n",
    "        test_errs.append(round( 1.0 - test_score,2))\n",
    "    \n",
    "    return train_errs,test_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3], [0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [[44, 15],[75, 17],[30, 50],[41, 89],[15,3],[36,  7],[46, 22],[96, 13],[85,  5],[81, 35]]\n",
    "y_train = [1, 1, 1, 0, 1, 1, 0, 1, 0, 0]\n",
    "X_test = [[61, 64],[40, 91],[67, 22],[81, 13],[50, 85],[30, 49],[35, 10],[49, 76],[50, 65],[56,  2]]\n",
    "y_test = [0, 1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
    "c = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "\n",
    "training_errors(X_train, y_train, X_test, y_test,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e287c795756e004cbc239a0cd2370360bfb1c5c74b3b718d3796ac64793fd0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
